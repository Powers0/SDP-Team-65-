This module trains a neural network model to predict the next pitch type in an MLB at-bat using Statcast pitch-by-pitch data.
The system uses LSTM sequence modeling, learned pitcher and batter embeddings, and a fully modular preprocessing + training pipeline.

build_pitchtypedata.py
    Applies preprocessing
	â€¢	Converts baserunners â†’ 0/1
	â€¢	Computes score differential (score_diff)
	â€¢	Fixes missing zones (zone=-1)
	â€¢	One-hot encodes:
	â€¢	batter stance (stand)
	â€¢	pitcher throwing hand (p_throws)
	â€¢	previous pitch type (previous_pitch)
	â€¢	Computes previous pitch zone (previous_zone)
	â€¢	Builds:
	â€¢	pitcher_id = categorical integer code
	â€¢	batter_id = categorical integer code

    Creates fixed-length sequences
    For each at-bat: [pitch_t-5, pitch_t-4, pitch_t-3, pitch_t-2, pitch_t-1] â†’ pitch_type_t
    Saves reusable artifacts
	â€¢	features.pkl (column order)
	â€¢	scaler.pkl (fit ONLY on training data)
	â€¢	label_encoder.pkl
	â€¢	X_train / X_test
	â€¢	pitcher/batter ID sequences
	â€¢	pitchtype_model.keras (after training)

    These artifacts let the front-end run predictions without preprocessing the raw CSV data again.


train_pitchtypemodel.py
    Uses the artifact dataset and trains the LSTM + embedding model.

Key training features:
	â€¢	Train/test split (80/20)
	â€¢	Validation split (20%)
	â€¢	EarlyStopping(patience=3)
	â€¢	Accuracy, classification report, confusion matrix saved/displayed
	â€¢	Softmax probability output saved to pitch_type_probs.npy for the pitch-location model


Model Architecture pitch_model_lstm.py
    This architecture was designed for interpretable sequential modeling while embedding characteristics of pitchers and batters.

Inputs
	1.	X â†’ (batch, SEQ_LEN, num_features)
	2.	pitcher_id â†’ (batch, SEQ_LEN)
	3.	batter_id â†’ (batch, SEQ_LEN)

    Embeddings
    pitcher_emb = Embedding(num_pitchers, 8)
    batter_emb  = Embedding(num_batters, 8)
    Each pitcher/batter learns an 8-dimensional vector capturing tendencies such as:
	â€¢	pitch repertoire & sequencing
	â€¢	release consistency
	â€¢	batter hot/cold zones
	â€¢	handedness effects

    Concatenation

    At each timestep:
        [features | pitcher_emb | batter_emb]
    Masking(mask_value=0.0)
    LSTM(128)
    Dropout(0.3)
    Dense(64, relu)
    Dense(n_classes, softmax)

    Training
	â€¢	Loss: categorical crossentropy
	â€¢	Optimizer: Adam
	â€¢	EarlyStopping: monitors val_loss

    Why these choices?
    Embedding(8)
    Large enough to encode tendencies, small enough to prevent overfitting
    LSTM(128)
    Balanced capacity for pitch sequencing patterns
    Dropout(0.3)
    Prevents overfitting in sequential model
    Dense(64)
    Intermediate abstraction for final classification
    Softmax
    Multi-class pitch prediction requires probability distribution

ðŸ“Š Final Model Performance

Typical evaluation:
	â€¢	Accuracy ~52â€“53% on real-world 7-class pitch set
	â€¢	Good performance on FF, SI, FC
	â€¢	Lower performance on CU, ST (class imbalance)

